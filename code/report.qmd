---
title: "Assignment 1"
author: "JRGLAY001, GMDSBO006 and MGRKHE001"
date: today
format: 
  pdf:
    pdfengine: pdflatex
    include-in-header: 
      text: |
        \usepackage[a4paper, left=0.3in, right=0.3in, top=0.3in, bottom=0.3in]{geometry}
        \extrafloats{500}
        \usepackage{etex}
        \usepackage{tabularx}
        \maxdeadcycles=1000
        \extrarowheight=5pt
        \maxdimen=16383.99999pt
        \renewcommand{\baselinestretch}{0.65}
        \newcommand{\B}[1]{\mathbf{#1}}
---

::: {#fig-dm}

![](../images/cooking.jpg)

Let me cook!

:::

\newpage

# Question 1
## a)

Model formulation
$$y = X\beta + e$$
where $$e \sim \mathcal{N}(0, \sigma^2 I_{n})$$

$$\mathbb{E}[y] = \mathbb{E}[X\beta + e]
	= \mathbb{E}[X\beta] + \mathbb{E}[e]
	= X\beta + 0
	= X\beta$$
$$\mathrm{Var}[y] = \mathrm{Var}[X\beta + e]
	= \mathrm{Var}[X\beta] + \mathrm{Var}[e] + 2cov(X\beta, e)
	= 0 + \sigma^2I_{n} + 0
	= \sigma^2I_{n}$$

Therefore $$y \sim \mathcal{N}(X\beta, \sigma^2I_{n})$$

Density of Y
$$f_{Y}(y) = (2\pi\sigma^2)^{-\frac{n}{2}} exp(-\frac{1}{2\sigma^2} (y - X\beta)^{T}(y - X\beta))$$
$$f_{Y}(y) = (2\pi\sigma^2)^{-\frac{n}{2}} exp(-\frac{1}{2\sigma^2} (y^{T}y - 2\beta^{T}X^{T}y + \beta^{T} X^{T}X \beta))$$

Likelihood
$$L(\beta, y, X) \propto (\sigma^2)^{-\frac{n}{2}} exp(-\frac{1}{2\sigma^2} (y^{T}y - 2\beta^{T}X^{T}y + \beta^{T}X^{T}X\beta))$$

Since $M = M^{-1} = I_{k+1}$

Then the prior distribution is
$$[\beta|\sigma^2] \sim \mathcal{N}_{k+1}(\tilde{\beta}, \sigma^2 M^{-1})$$

Prior density
$$\pi(\beta|\sigma^2)
	= (2\pi)^{-\frac{k+1}{2}}
	\det(\sigma^2M^{-1})^{-\frac{1}{2}}
	exp(-\frac{1}{2\sigma^2}(\beta - \tilde{\beta})^{T} M (\beta - \tilde{\beta}))$$
$$\pi(\beta|\sigma^2)
	= (2\pi)^{-\frac{k+1}{2}}
	\det(\sigma^2M^{-1})^{-\frac{1}{2}}
	exp(-\frac{1}{2\sigma^2}(\beta^{T}M\beta -2\beta^{T}M\tilde{\beta} + \tilde{\beta^{T}}M\tilde{\beta}))$$
$$\pi(\beta|\sigma^2)
	\propto  exp(-\frac{1}{2\sigma^2}(\beta^{T}M\beta -2\beta^{T}M\tilde{\beta} + \tilde{\beta^{T}}M\tilde{\beta}))$$

Prior distribution
$$[\sigma^2] \sim \mathcal{IG}(a, b)$$

Prior density

$$\pi(\sigma^2)
	= \frac{b^{a}}{\Gamma{(a)}} \sigma^{2(-a-1)} exp(-\frac{b}{\sigma^2})$$
$$\pi(\sigma^2)
	\propto \sigma^{2(-a-1)} exp(-\frac{b}{\sigma^2})$$

\newpage

\textbf{Assuming that $\sigma^2$ is known}

Prior
$$\pi(\beta|\sigma^2)
	\propto exp(-\frac{1}{2\sigma^2}(\beta^{T}M\beta -2\beta^{T}M\tilde{\beta}))$$

Likelihood
$$L(\beta|\sigma^2)
	\propto exp(-\frac{1}{2\sigma^2} (-2\beta^{T}X^{T}y + \beta^{T}X^{T}X\beta))$$

Posterior
$$\pi(\beta| y, X)
\propto exp(-\frac{1}{2\sigma^2} (\beta^{T}(M + X^{T}X)\beta -2\beta^{T}(X^{T}y + M\tilde{\beta})))$$

Recall that $(X^{T}X) \hat{\beta} = X^{T}y$
$$\pi(\beta| y, X)
	\propto exp(-\frac{1}{2\sigma^2} (\beta^{T}(M + X^{T}X)\beta -2\beta^{T}((X^{T}X) \hat{\beta} + M\tilde{\beta})))$$

Let $A = M + X^{T}X$ and $b = (X^{T}X) \hat{\beta} + M\tilde{\beta}$

Completing the square using the following identity
$$\beta^{T}A\beta - 2\beta^{T}b
	= (\beta - \mu_{\beta})^{T}A(\beta - \mu_{\beta}) - \mu_{\beta}^{T}A\mu_{\beta}$$ 
where $\mu_{\beta} = A^{-1}b$

$$\pi(\beta| y, X)
	\propto exp(-\frac{1}{2\sigma^2} 
	((\beta - \mu_{\beta})^{T}A(\beta - \mu_{\beta}) - \mu_{\beta}^{T}A\mu_{\beta}))$$

but $\mu_{\beta}^{T}A\mu_{\beta}$ is independent of $\beta$
$$\pi(\beta| y, X) \propto exp(-\frac{1}{2\sigma^2} (\beta - \mu_{\beta})^{T}A(\beta - \mu_{\beta}))$$
$$\pi(\beta| y, X) \propto exp(-\frac{1}{2\sigma^2} (\beta - \mu_{\beta})^{T}(M + X^{T}X)(\beta - \mu_{\beta}))$$

meaning $$[\beta| \sigma^2, y, X] \sim \mathcal{N}_{k+1}(\mu_{\beta}, \sigma^2(M + X^{T}X)^{-1})$$
where $$\mu_{\beta} = (M + X^{T}X)^{-1} ((X^{T}X) \hat{\beta} + M\tilde{\beta})$$

\newpage

Joint distribution
$$J = [\beta, \sigma^2| y, X]$$

Consider
$$P = L(\beta, y, X)[\beta| \sigma^2]$$
$$P \propto (\sigma^2)^{-\frac{n}{2}} 
	exp(-\frac{1}{2\sigma^2}(y^{T}y -2\beta^{T}X^{T}y + \beta^{T}X^{T}X\beta)) 
	(\sigma^2)^{-\frac{k+1}{2}}
	exp(-\frac{1}{2\sigma^2}(\beta^{T}M\beta - 2\beta^{T}M\tilde{\beta} + \tilde{\beta^{T}}M\tilde{\beta}))$$
$$P \propto (\sigma^2)^{-\frac{n+k+1}{2}} 
	exp(-\frac{1}{2\sigma^2} (y^{T}y + \tilde{\beta^{T}}M\tilde{\beta}))
	exp(-\frac{1}{2\sigma^2} (\beta^{T}(M + X^{T}X)\beta -2\beta^{T}((X^{T}X) \hat{\beta} + M\tilde{\beta}))$$
Let $A = M + X^{T}X$ and $b = (X^{T}X) \hat{\beta} + M\tilde{\beta}$

Completing the square using the following identity
$$\beta^{T}A\beta - 2\beta^{T}b
	= (\beta - \mu_{\beta})^{T}A(\beta - \mu_{\beta}) - \mu_{\beta}^{T}A\mu_{\beta}$$ 
where $\mu_{\beta} = A^{-1}b$
$$P \propto (\sigma^2)^{-\frac{n+k+1}{2}} 
	exp(-\frac{1}{2\sigma^2} (y^{T}y + \tilde{\beta^{T}}M\tilde{\beta}))
	exp(-\frac{1}{2\sigma^2} ((\beta - \mu_{\beta})^{T}A(\beta - \mu_{\beta}) - \mu_{\beta}^{T}A\mu_{\beta}))$$
$$P \propto (\sigma^2)^{-\frac{n+k+1}{2}} 
	exp(-\frac{1}{2\sigma^2} (y^{T}y + \tilde{\beta^{T}}M\tilde{\beta} - \mu_{\beta}^{T}A\mu_{\beta}))
	exp(-\frac{1}{2\sigma^2} (\beta - \mu_{\beta})^{T}A(\beta - \mu_{\beta}))$$

$$[\sigma^2]
	\propto \sigma^{2(-a-1)} exp(-\frac{b}{\sigma^2})$$

$$J \propto L(\beta, y, X)[\beta| \sigma^2][\sigma^2]$$
$$J \propto (\sigma^2)^{-(\frac{n+k+1}{2} + a + 1)} 
	exp(-\frac{1}{2\sigma^2} (y^{T}y + \tilde{\beta^{T}}M\tilde{\beta} - \mu_{\beta}^{T}A\mu_{\beta}))
	exp(-\frac{1}{2\sigma^2} (\beta - \mu_{\beta})^{T}A(\beta - \mu_{\beta}))
	exp(-\frac{b}{\sigma^2})$$
Let $A_{2} = y^{T}y + \tilde{\beta^{T}}M\tilde{\beta} - \mu_{\beta}^{T}A\mu_{\beta}$
$$J \propto (\sigma^2)^{-(\frac{n+k+1}{2} + a + 1)} 
	exp(-\frac{1}{\sigma^2} (b + \frac{A_{2}}{2}))
	exp(-\frac{1}{2\sigma^2} (\beta - \mu_{\beta})^{T}A(\beta - \mu_{\beta}))$$

$$[\sigma^2| y, X] = \int_{\beta} [\beta, \sigma^2| y, X]\, d\beta$$
$$[\sigma^2| y, X] \propto (\sigma^2)^{-(\frac{n+k+1}{2} + a + 1)} 
	exp(-\frac{1}{\sigma^2} (b + \frac{A_{2}}{2}))
	\int_{\beta}exp(-\frac{1}{2\sigma^2} (\beta - \mu_{\beta})^{T}A(\beta - \mu_{\beta}) \, d\beta$$
Recall
$$\int_{\beta}
	(2\pi)^{-\frac{k+1}{2}}det((\frac{1}{\sigma^2}(M + X^{T}X))^{-1})^{-\frac{1}{2}}
	exp(-\frac{1}{2\sigma^2} (\beta - \mu_{\beta})^{T}A(\beta - \mu_{\beta})) \, d\beta
	= 1$$
then
$$I = \int_{\beta}
	exp(-\frac{1}{2\sigma^2} (\beta - \mu_{\beta})^{T}A(\beta - \mu_{\beta}) \, d\beta 
	= (2\pi)^{\frac{k+1}{2}}det((\frac{1}{\sigma^2}(M + X^{T}X))^{-1})^{\frac{1}{2}})$$

We know that $det(aA) = a^{k}det(A)$, where $A$ is a $k$ by $k$ matrix.

$$I = (2\pi)^{\frac{k+1}{2}}
(\sigma^2)^{\frac{k+1}{2}}
det((M + X^{T}X)^{-1})^{\frac{1}{2}}$$
$$I \propto (\sigma^2)^\frac{k+1}{2}$$
$$[\sigma^2| y, X] \propto (\sigma^2)^{-(\frac{n+k+1}{2} + a + 1)} 
	exp(-\frac{1}{\sigma^2} (b + \frac{A_{2}}{2}))
	(\sigma^2)^{\frac{k+1}{2}}$$
$$[\sigma^2| y, X] \propto (\sigma^2)^{-(\frac{n}{2} + a) - 1} 
	exp(-\frac{1}{\sigma^2} (b + \frac{A_{2}}{2}))$$
meaning
$$[\sigma^2| y, X] \sim \mathcal{IG}(a + \frac{n}{2}, b + \frac{A_{2}}{2})$$
where $a = 1$, $b = 1$ and $A_{2} = y^{T}y + \tilde{\beta^{T}}M\tilde{\beta} - \mu_{\beta}^{T}(M + X^{T}X)\mu_{\beta}$

## b)
```{r setup, echo = FALSE, warning = FALSE}
# Packages required
suppressPackageStartupMessages(require(cubature))
suppressPackageStartupMessages(require(knitr))
suppressPackageStartupMessages(require(MASS))
suppressPackageStartupMessages(require(MCMCpack))
```

```{r data, echo = FALSE}
# Lets simulate some data
set.seed(2021)

n <- 150 # Number of data points
X.c <- data.frame(matrix(rnorm(5*n), ncol=5))
colnames(X.c) <-  c("X1","X2","X3","X4", "X5")
X <- as.matrix(cbind(1, X.c)) # Design matrix
e <- matrix(rnorm(n), ncol=1) # Errors
beta.true <- matrix(c(1,0,10,0,2,-3), ncol=1)
Y <- X%*%beta.true + e # Observations

k <- nrow(beta.true)
a <- 1
b <- 1
M <- diag(nrow = k)
beta.hat <- solve(t(X)%*%X) %*% t(X) %*% Y
mu.beta <- solve(M + t(X)%*%X) %*% ((t(X)%*%X) %*% beta.hat)
A2 <- t(Y) %*% Y - t(mu.beta) %*% (M + t(X)%*%X) %*% mu.beta

palette <- c('lightskyblue', 'lightseagreen', 'palegreen', 'plum3', 'lightsalmon', 
				 'khaki2', 'black', 'firebrick', 'hotpink')
```

```{r IG, echo = FALSE}
# Sampling from the Inverse Gamma
Sigma2.samples <- rinvgamma(n=50000, shape=(a + n/2), scale=(b + A2/2))

kable(head(Sigma2.samples), 
		digits = 7,
		caption = 'First six rows of $\\sigma^2$ sample values', 
		col.names = c('$\\sigma^2$'))
```

```{r sigma, echo = FALSE}
# Inverse Gamma Sample
hist(x = Sigma2.samples,
	  main = expression(paste('Histogram of ', sigma^2)), 
	  xlab = expression(sigma^2),
	  ylab = 'Density',
	  col = palette[1],
	  freq = FALSE,
	  breaks = 30)
```

```{r MVN, echo = FALSE}
# Sampling from the Multivariate Normal distribution
MVN <- function(S, E, A){
	# S is the \sigma^2 sample 
	# E is the mean vector
	# A = (M + X^{T}X)^{-1}
	n <- length(S) # \sigma^2 sample size
	MAT <- matrix(0, nrow=n, ncol=6) # Stores the generated samples
	for(i in 1:n){
		# For each sigma^2 we condition on it to obtain a beta estimate
		MAT[i, ] <- mvrnorm(n=1, mu=E, Sigma=S[i]*A)
	}
	return(MAT)
}

# Beta sample
A <- solve(M + t(X) %*% X)
Beta.samples <- mcmc(MVN(S=Sigma2.samples, E=mu.beta, A=A))

col_labels <- c("$\\beta_0$", "$\\beta_1$", "$\\beta_2$", 
					 "$\\beta_3$", "$\\beta_4$", "$\\beta_5$")

kable(head(Beta.samples), 
		digits = 3, 
		caption = 'First six rows of the $\\beta$ sample values', 
		col.names = col_labels)
```

\newpage

## c)
### i)
```{r trace, echo = FALSE, fig.width=8, fig.height=10}
# Trace plots
par(mfrow = c(3, 2))
for(i in 1:6){
	traceplot(x = Beta.samples[, i], 
				 main = bquote('Trace plot of ' ~ beta[.(i-1)]),
				 col = palette[i])
}
```

All of the trace plots appear as random scatter, indicating stationarity. This provides evidence for the convergence of the Markov Chains.

\newpage

### ii)
```{r density, echo = FALSE, fig.width=8, fig.height=10}
# Density plots
par(mfrow = c(3, 2))
for(i in 1:6){
	hist(x = Beta.samples[, i],
		  main = bquote('Histogram of' ~ beta[.(i-1)]),
		  xlab = bquote(beta[.(i-1)]),
		  ylab = 'Density',
		  col = palette[i],
		  freq = FALSE,
		  breaks = 30)
	abline(v = mean(Beta.samples[, i]), col = palette[7], lwd = 2)
	abline(v = beta.true[i], col = palette[8], lwd = 2)
	abline(v = quantile(x = Beta.samples[, i], 0.025), col = palette[9], lwd = 2)
	abline(v = quantile(x = Beta.samples[, i], 0.975), col = palette[9], lwd = 2)
	legend(x = 'topright', 
			 legend = c(bquote('Sample avarage' ~ beta[.(i-1)]), 
			 			  bquote('True' ~ beta[.(i-1)]), 
			 			  '95% credibility interval'),
			 col = palette[7:9],
			 lty = 1,
			 lwd = 2)
}
```
The beta coefficients are approximately normally distributed.

Confidence interval are the relative frequencies of stating valid bounds if you were to re-sample the data.
Credibility intervals are the probability that the true parameter is within the stated bounds.

\newpage

# Question 2

