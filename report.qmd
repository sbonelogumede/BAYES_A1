---
title: "Assignment 1"
author:
  - Jorge Abrahams Laylaa (JRGLAY001)
  - Gumede Sbonelo (GMDSBO006)
  - Mugeri Khethani (MGRKHE001)
date: today
format: 
  pdf:
    pdfengine: pdflatex
    include-in-header: 
      text: |
        \makeatletter
        \@fleqntrue
        \makeatother
        \usepackage[a4paper, left=0.3in, right=0.3in, top=0.3in, bottom=0.3in]{geometry}
        \extrafloats{500}
        \usepackage{etex}
        \usepackage{tabularx}
        \usepackage{hyperref}
        \maxdeadcycles=1000
        \extrarowheight=5pt
        \maxdimen=16383.99999pt
        \renewcommand{\baselinestretch}{0.65}
        \newcommand{\F}[1]{\mathbf{#1}}
        \newcommand{\B}[1]{\symbf{#1}}
        \newcommand{\bm}[1]{\symbf{#1}}
editor: 
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: console
---
# Question 1

## (a)

Model formulation for $\F{y}$:

$\F{y} = \F{X}\B{\beta} + \F{e}$ where $\F{e} \sim \mathcal{N}(\F{0}, \sigma^2 \F{I}_{n})$

Moments for $\F{y}$:
$$\mathbb{E}[\F{y}] = \mathbb{E}[\F{X}\B{\beta} + \F{e}]
    = \mathbb{E}[\F{X}\B{\beta}] + \mathbb{E}[\F{e}]
    = \F{X}\B{\beta} + \F{0}
    = \F{X}\B{\beta}$$
$$\mathrm{Var}[\F{y}] = \mathrm{Var}[\F{X}\B{\beta} + \F{e}]
    = \mathrm{Var}[\F{X}\B{\beta}] + \mathrm{Var}[\F{e}] + 2cov(\F{X}\B{\beta}, \F{e})
    = \F{0} + \sigma^2\F{I}_{n} + \F{0}
    = \sigma^2\F{I}_{n}$$

Distribution for $\F{y}$:
$$\F{y} \sim \mathcal{N}(\F{X}\B{\beta}, \sigma^2\F{I}_{n})$$

Density for $\F{y}$:
$$f_{Y}(\F{y}) = (2\pi\sigma^2)^{-\frac{n}{2}}
exp(-\frac{1}{2\sigma^2} (\F{y} - \F{X}\B{\beta})^{T}(\F{y} - \F{X}\B{\beta}))$$
$$f_{Y}(\F{y}) = (2\pi\sigma^2)^{-\frac{n}{2}}
exp(-\frac{1}{2\sigma^2} (\F{y}^{T}\F{y} - 2\B{\beta}^{T}\F{X}^{T}\F{y} + \B{\beta}^{T} \F{X}^{T}\F{X} \B{\beta}))$$

Likelihood for $\B{\beta}$:
$$L(\B{\beta}, \F{y}, \F{X}) \propto (\sigma^2)^{-\frac{n}{2}} 
exp(-\frac{1}{2\sigma^2} (\F{y}^{T}\F{y} - 2\B{\beta}^{T}\F{X}^{T}\F{y} + \B{\beta}^{T}\F{X}^{T}\F{X}\B{\beta}))$$

Since $\F{M} = \F{M}^{-1} = \F{I}_{k+1}$

Prior distribution for $\B{\beta} | \sigma^2$:
$$[\B{\beta} | \sigma^2] \sim \mathcal{N}_{k+1} (\tilde{\B{\beta}}, \sigma^2 \F{M}^{-1})$$

Prior density for $\B{\beta} | \sigma^2$:
$$\pi(\B{\beta} | \sigma^2)
    = (2\pi)^{-\frac{k+1}{2}}
    \det(\sigma^2 \F{M}^{-1})^{-\frac{1}{2}}
    exp(-\frac{1}{2\sigma^2} (\B{\beta} - \tilde{\B{\beta}})^{T} \F{M} (\B{\beta} - \tilde{\B{\beta}}))$$
$$\pi(\B{\beta} | \sigma^2)
    = (2\pi)^{-\frac{k+1}{2}}
    \det(\sigma^2 \F{M}^{-1})^{-\frac{1}{2}}
    exp(-\frac{1}{2\sigma^2}(\B{\beta}^{T} \F{M} \B{\beta} -2 \B{\beta}^{T} \F{M} \tilde{\B{\beta}} + \tilde{\B{\beta}}^{T} \F{M} \tilde{\B{\beta}}))$$
$$\pi(\B{\beta}|\sigma^2)
    \propto  exp(-\frac{1}{2\sigma^2} (\B{\beta}^{T} \F{M} \B{\beta} -2 \B{\beta}^{T} \F{M} \B{\tilde{\beta}} + \tilde{\B{\beta}}^{T} \F{M} \tilde{\B{\beta}}))$$

Prior distribution for $\sigma^2$:
$$[\sigma^2] \sim \mathcal{IG}(a, b)$$

Prior density for $\sigma^2$:
$$\pi(\sigma^2) = \frac{b^{a}}{\Gamma{(a)}} \sigma^{2(-a-1)} exp(-\frac{b}{\sigma^2})$$
$$\pi(\sigma^2) \propto \sigma^{2(-a-1)} exp(-\frac{b}{\sigma^2})$$

### i.
Assuming that $\sigma^2$ is known:

Prior density for $\B{\beta}|\sigma^2$:
$$\pi(\B{\beta} | \sigma^2) \propto exp(-\frac{1}{2\sigma^2}(\B{\beta}^{T} \F{M} \B{\beta} -2 \B{\beta}^{T} \F{M} \tilde{\B{\beta}}))$$

Likelihood for  $\B{\beta}|\sigma^2$:
$$L(\B{\beta} | \sigma^2) \propto exp(-\frac{1}{2\sigma^2} (-2\B{\beta}^{T} \F{X}^{T}\B{y} + \B{\beta}^{T} \F{X}^{T} \F{X} \B{\beta}))$$

Posterior for  $\B{\beta}|\sigma^2$:
$$\pi(\B{\beta}| \F{y}, \F{X}) 
\propto exp(-\frac{1}{2\sigma^2} (\B{\beta}^{T}(\F{M} 
+ \F{X}^{T} \F{X}) \B{\beta}
- 2 \B{\beta}^{T} (\F{X}^{T} \F{y} + \F{M} \tilde{\B{\beta}})))$$
but $(\F{X}^{T} \F{X}) \hat{\B{\beta}} = \F{X}^{T} \F{y}$
$$\pi(\B{\beta}| \F{y}, \F{X})
\propto exp(-\frac{1}{2\sigma^2} (\B{\beta}^{T} (\F{M} + \B{X}^{T} \F{X}) \B{\beta} 
- 2 \B{\beta}^{T} ((\F{X}^{T} \F{X}) \hat{\B{\beta}} + \F{M} \tilde{\B{\beta}})))$$

Let $\F{A} = \F{M} + \F{X}^{T} \F{X}$ and $\F{b} = (\F{X}^{T} \F{X}) \hat{\B{\beta}} + \F{M} \tilde{\B{\beta}}$

Completing the square:
$\B{\beta}^{T} \F{A} \B{\beta} - 2 \B{\beta}^{T} \F{b} = (\B{\beta} - \mu_{\B{\beta}})^{T} \F{A} (\B{\beta} - \mu_{\B{\beta}}) - \mu_{\B{\beta}}^{T} \F{A} \mu_{\B{\beta}}$, where $\mu_{\B{\beta}} = \F{A}^{-1} \F{b}$
$$\pi(\B{\beta}| \F{y}, \F{X})
\propto exp(-\frac{1}{2\sigma^2} 
((\B{\beta} - \mu_{\B{\beta}})^{T} \F{A} (\B{\beta} - \mu_{\B{\beta}}) - \mu_{\B{\beta}}^{T} \F{A} \mu_{\B{\beta}}))$$
but $\mu_{\B{\beta}}^{T} \F{A} \mu_{\B{\beta}}$ is independent of $\B{\beta}$
$$\pi(\B{\beta}| \F{y}, \F{X}) \propto exp(-\frac{1}{2\sigma^2} (\B{\beta} - \mu_{\B{\beta}})^{T} \F{A} (\B{\beta} - \mu_{\B{\beta}}))$$
$$\pi(\B{\beta}| \F{y}, \F{X}) \propto exp(-\frac{1}{2\sigma^2} (\B{\beta} - \mu_{\B{\beta}})^{T}( \F{M} + \F{X}^{T} \F{X})( \B{\beta} - \mu_{\B{\beta}}))$$
Therefore:

$[\B{\beta}| \sigma^2, \B{y}, \B{X}] \sim \mathcal{N}_{k+1}(\mu_{\B{\beta}}, \sigma^2(\F{M} + \F{X}^{T} \F{X})^{-1})$, where $\mu_{\B{\beta}} = (\F{M} + \F{X}^{T} \F{X})^{-1} ((\F{X}^{T} \F{X}) \hat{\B{\beta}} + \F{M} \tilde{\B{\beta}})$

### ii.

Consider $L(\B{\beta}, \F{y}, \F{X})[\B{\beta} | \sigma^2]$:
$$L(\B{\beta}, \F{y}, \F{X})[\B{\beta} | \sigma^2]
\propto (\sigma^2)^{-\frac{n}{2}} 
exp(-\frac{1}{2\sigma^2}(\F{y}^{T} \F{y} - 2 \B{\beta}^{T} \F{X}^{T} \F{y} + \B{\beta}^{T} \F{X}^{T} \F{X} \B{\beta})) 
(\sigma^2)^{-\frac{k+1}{2}}
exp(-\frac{1}{2\sigma^2}(\B{\beta}^{T} \F{M} \B{\beta} - 2 \B{\beta}^{T} \F{M} \tilde{\B{\beta}} + \tilde{\B{\beta}}^{T} \F{M} \tilde{\B{\beta}}))$$
$$L(\B{\beta}, \F{y}, \F{X})[\B{\beta} | \sigma^2]
\propto (\sigma^2)^{-\frac{n+k+1}{2}} 
exp(-\frac{1}{2\sigma^2} (\F{y}^{T} \F{y} + \tilde{\B{\beta}}^{T} \F{M} \tilde{\B{\beta}}))
exp(-\frac{1}{2\sigma^2} (\B{\beta}^{T}( \F{M} + \F{X}^{T} \F{X}) \B{\beta} - 2 \B{\beta}^{T} ((\F{X}^{T} \F{X}) \hat{\B{\beta}} + \F{M} \tilde{\B{\beta}}))$$

Let $\F{A} = \F{M} + \F{X}^{T} \F{X}$ and $\F{b} = (\F{X}^{T} \F{X}) \hat{\B{\beta}} + \F{M} \tilde{\B{\beta}}$

Completing the square:
$\B{\beta}^{T} \F{A} \B{\beta} - 2 \B{\beta}^{T} \F{b} = (\B{\beta} - \mu_{\B{\beta}})^{T} \F{A} (\B{\beta} - \mu_{\B{\beta}}) - \mu_{\B{\beta}}^{T} \F{A} \mu_{\B{\beta}}$, where $\mu_{\B{\beta}} = \F{A}^{-1} \F{b}$
$$L(\B{\beta}, \F{y}, \F{X})[\B{\beta} | \sigma^2]
\propto (\sigma^2)^{-\frac{n+k+1}{2}} 
exp(-\frac{1}{2\sigma^2} (\F{y}^{T} \F{y} + \tilde{\B{\beta}}^{T} \F{M} \tilde{\B{\beta}}))
exp(-\frac{1}{2\sigma^2} ((\B{\beta} - \mu_{\B{\beta}})^{T} \F{A} (\B{\beta} - \mu_{\B{\beta}}) - \mu_{\B{\beta}}^{T} \F{A} \mu_{\B{\beta}}))$$
$$L(\B{\beta}, \F{y}, \F{X})[\B{\beta} | \sigma^2]
\propto (\sigma^2)^{-\frac{n+k+1}{2}} 
exp(-\frac{1}{2\sigma^2} (\F{y}^{T} \F{y} + \tilde{\B{\beta}}^{T} \F{M} \tilde{\B{\beta}} - \mu_{\B{\beta}}^{T} \F{A} \mu_{\B{\beta}}))
exp(-\frac{1}{2\sigma^2} (\B{\beta} - \mu_{\B{\beta}})^{T} \F{A} (\B{\beta} - \mu_{\B{\beta}}))$$
Prior density for $\sigma^2$:
$$[\sigma^2] \propto \sigma^{2(-a-1)} exp(-\frac{b}{\sigma^2})$$
$$L(\B{\beta}, \F{y}, \F{X})[\B{\beta} | \sigma^2][\sigma^2]
\propto (\sigma^2)^{-(\frac{n+k+1}{2} + a + 1)} 
exp(-\frac{1}{2\sigma^2} (\F{y}^{T} \F{y} + \tilde{\B{\beta}}^{T} \F{M} \tilde{\B{\beta}} - \mu_{\B{\beta}}^{T} \F{A} \mu_{\B{\beta}}))
exp(-\frac{1}{2\sigma^2} (\B{\beta} - \mu_{\B{\beta}})^{T} \F{A} (\B{\beta} - \mu_{\B{\beta}}))
exp(-\frac{b}{\sigma^2})$$
Let $A_{2} = \F{y}^{T} \F{y} + \tilde{\B{\beta}}^{T} \F{M} \tilde{\B{\beta}} - \mu_{\B{\beta}}^{T} \F{A} \mu_{\B{\beta}}$
$$L(\B{\beta}, \F{y}, \F{X})[\B{\beta} | \sigma^2][\sigma^2] 
\propto (\sigma^2)^{-(\frac{n+k+1}{2} + a + 1)} 
exp(-\frac{1}{\sigma^2} (b + \frac{A_{2}}{2}))
exp(-\frac{1}{2\sigma^2} (\B{\beta} - \mu_{\B{\beta}})^{T} \B{A} (\B{\beta} - \mu_{\B{\beta}}))$$
$$[\sigma^2| \F{y}, \F{X}] = \int_{\B{\beta}} [\B{\beta}, \sigma^2| \F{y}, \F{X}]\, d\B{\beta}$$
$$[\sigma^2| \F{y}, \F{X}] \propto (\sigma^2)^{-(\frac{n+k+1}{2} + a + 1)} 
exp(-\frac{1}{\sigma^2} (b + \frac{A_{2}}{2}))
\int_{\B{\beta}}exp(-\frac{1}{2\sigma^2} (\B{\beta} - \mu_{\B{\beta}})^{T} \F{A} (\B{\beta} - \mu_{\B{\beta}}) \, d\B{\beta}$$
$$\int_{\B{\beta}}
(2\pi)^{-\frac{k+1}{2}}det((\frac{1}{\sigma^2}(\F{M} + \F{X}^{T} \F{X}))^{-1})^{-\frac{1}{2}}
exp(-\frac{1}{2\sigma^2} (\B{\beta} - \mu_{\B{\beta}})^{T} \F{A} (\B{\beta} - \mu_{\B{\beta}})) \, d\B{\beta}
= 1$$
$$I = \int_{\B{\beta}}
exp(-\frac{1}{2\sigma^2} (\B{\beta} - \mu_{\B{\beta}})^{T} \F{A} (\B{\beta} - \mu_{\B{\beta}}) \, d\B{\beta} 
= (2\pi)^{\frac{k+1}{2}}det((\frac{1}{\sigma^2}(\F{M} + \F{X}^{T} \F{X}))^{-1})^{\frac{1}{2}})$$
We know that $det(a \F{A}) = a^{k}det(\F{A})$, where $\F{A}$ is a $k$ by $k$ matrix.
$$I = (2\pi)^{\frac{k+1}{2}}
(\sigma^2)^{\frac{k+1}{2}}
det((\F{M} + \F{X}^{T} \F{X})^{-1})^{\frac{1}{2}}$$
$$I \propto (\sigma^2)^\frac{k+1}{2}$$
$$[\sigma^2| \F{y}, \F{X}]
\propto (\sigma^2)^{-(\frac{n+k+1}{2} + a + 1)} 
exp(-\frac{1}{\sigma^2} (b + \frac{A_{2}}{2}))
(\sigma^2)^{\frac{k+1}{2}}$$
$$[\sigma^2| \F{y}, \F{X}] 
\propto (\sigma^2)^{-(\frac{n}{2} + a) - 1} 
exp(-\frac{1}{\sigma^2} (b + \frac{A_{2}}{2}))$$ 
Therefore:

$[\sigma^2| \F{y}, \F{X}] \sim \mathcal{IG}(a + \frac{n}{2}, b + \frac{A_{2}}{2})$, where $a = 1$, $b = 1$ and
$A_{2} = \F{y}^{T} \F{y} + \tilde{\B{\beta}}^{T} \F{M} \tilde{\B{\beta}} - \mu_{\B{\beta}}^{T} (\F{M} + \F{X}^{T} \F{X}) \mu_{\B{\beta}}$

## (b)

```{r setup, echo = FALSE, warning = FALSE}
# Packages required
suppressPackageStartupMessages(require(cubature))
suppressPackageStartupMessages(require(knitr))
suppressPackageStartupMessages(require(ggplot2))
suppressPackageStartupMessages(require(MASS))
suppressPackageStartupMessages(require(MCMCpack))
suppressMessages(require(patchwork))
```

```{r data, echo = FALSE}
# Lets simulate some data
set.seed(2021)

n <- 150 # Number of data points
X.c <- data.frame(matrix(rnorm(5*n), ncol=5))
colnames(X.c) <-  c("X1","X2","X3","X4", "X5")
X <- as.matrix(cbind(1, X.c)) # Design matrix
e <- matrix(rnorm(n), ncol=1) # Errors
beta.true <- matrix(c(1,0,10,0,2,-3), ncol=1)
Y <- X%*%beta.true + e # Observations

k <- nrow(beta.true)
a <- 1
b <- 1
M <- diag(nrow = k)
beta.hat <- solve(t(X)%*%X) %*% t(X) %*% Y
mu.beta <- solve(M + t(X)%*%X) %*% ((t(X)%*%X) %*% beta.hat)
A2 <- t(Y) %*% Y - t(mu.beta) %*% (M + t(X)%*%X) %*% mu.beta

palette <- c('lightskyblue', 'lightseagreen', 'palegreen', 'plum3', 'lightsalmon', 
				 'khaki2', 'black', 'firebrick', 'hotpink')
```

```{r IG, echo = FALSE}
# Sampling from the Inverse Gamma
Sigma2.samples <- rinvgamma(n=50000, shape=(a + n/2), scale=(b + A2/2))

kable(head(Sigma2.samples), 
		digits = 3,
		caption = 'First six rows of $\\sigma^2$ sample values', 
		col.names = c('$\\sigma^2$'))
```

```{r sigma, echo = FALSE}
# Inverse Gamma Sample
hist(x = Sigma2.samples,
	  main = expression(paste('Histogram of ', sigma^2)), 
	  xlab = expression(sigma^2),
	  ylab = 'Density',
	  col = palette[1],
	  freq = FALSE,
	  breaks = 30)
```

```{r MVN, echo = FALSE}
# Sampling from the Multivariate Normal distribution
MVN <- function(S, E, A){
	# S is the \sigma^2 sample 
	# E is the mean vector
	# A = (M + X^{T}X)^{-1}
	n <- length(S) # \sigma^2 sample size
	MAT <- matrix(0, nrow=n, ncol=6) # Stores the generated samples
	for(i in 1:n){
		# For each sigma^2 we condition on it to obtain a beta estimate
		MAT[i, ] <- mvrnorm(n=1, mu=E, Sigma=S[i]*A)
	}
	return(MAT)
}

# Beta sample
A <- solve(M + t(X) %*% X)
Beta.samples <- mcmc(MVN(S=Sigma2.samples, E=mu.beta, A=A))

col_labels <- c("$\\beta_0$", "$\\beta_1$", "$\\beta_2$", 
					 "$\\beta_3$", "$\\beta_4$", "$\\beta_5$")

kable(head(Beta.samples), 
		digits = 3, 
		caption = 'First six rows of the $\\beta$ sample values', 
		col.names = col_labels)
```

## (c)

### (i)

```{r trace, echo = FALSE, fig.width=8, fig.height=10}
# Trace plots
par(mfrow = c(3, 2))
for(i in 1:6){
	traceplot(x = Beta.samples[, i], 
				 main = bquote('Trace plot of ' ~ beta[.(i-1)]),
				 col = palette[i])
}
```

All of the trace plots appear as random scatter, indicating
stationarity. This provides evidence for the convergence of the Markov
Chains. Thus, the sampling quality appears to be good.

### (ii)

```{r density, echo = FALSE, fig.width=8, fig.height=10}
# Density plots
par(mfrow = c(3, 2))
for(i in 1:6){
	hist(x = Beta.samples[, i],
		  main = bquote('Histogram of' ~ beta[.(i-1)]),
		  xlab = bquote(beta[.(i-1)]),
		  ylab = 'Density',
		  col = palette[i],
		  freq = FALSE,
		  breaks = 30)
	abline(v = mean(Beta.samples[, i]), col = palette[7], lwd = 2)
	abline(v = beta.true[i], col = palette[8], lwd = 2)
	abline(v = quantile(x = Beta.samples[, i], 0.025), col = palette[9], lwd = 2)
	abline(v = quantile(x = Beta.samples[, i], 0.975), col = palette[9], lwd = 2)
	legend(x = 'topright', 
			 legend = c(bquote('Sample avarage' ~ beta[.(i-1)]), 
			 			  bquote('True' ~ beta[.(i-1)]), 
			 			  '95% credibility interval'),
			 col = palette[7:9],
			 lty = 1,
			 lwd = 2)
}
```

#### i.
The posterior distributions of the beta coefficients appear to be approximately normal. However, there seems to be a substantial deviation between the sample mean of the posterior and the true beta coefficient for $\beta_1$, $\beta_2$, and $\beta_4$. This suggests potential bias or high uncertainty in the estimation of these coefficients.

#### ii.
Confidence intervals represent the relative frequency with which the interval would contain the true parameter if the data were repeatedly resampled. Credible intervals, on the other hand, represent the probability that the true parameter lies within the stated bounds, given the observed data. The true beta parameter values are expected to lie within the credible intervals shown on the plot.

#### iii.
The credible intervals for $\beta_1$ and $\beta_3$ include zero, indicating little evidence that these coefficients differ significantly from zero. Therefore, there is weak evidence for retaining these variables in the model. In contrast, the credible intervals for $\beta_0$, $\beta_2$, $\beta_4$, and $\beta_5$ do not include zero, providing strong evidence that these variables have a meaningful effect and should be retained in the model.


# Question 2

## a)

Given:

$Z_i|Y_i \sim{Ber}(Y_ip_i)=(Y_ip_i)^z(1-y_ip_i)^{1-z}$

$Y_i \sim {Ber}(\theta_i)=(\theta_i)^y(1-\theta_i)^{1-y}$

Derivation of equation 2:

$P(Z_i=0)=\sum_{y=0}^{1}P(Z_i=0|Y_i=y)P(Y_i=y)$

$=(p_i)^0(1-pi)^1(\theta_i)^1(1-\theta_i)^0+(0p_i)^0(1-0p_i)^1(\theta_i)^0(1-\theta_i)^1$

$=(1-p_i)(\theta_i) +(1-\theta_i)$ $=1-p_i\theta_i$

$P(Y_i=1)=(\theta_i)^1(1-\theta_i)^0$

$=\theta_i$

$P(Z_i=0|Y_i=1)=(p_i)^0(1-p_i)^1$

Therefore :

$P(Y_i=1|Z_i=0) =\frac{P(Z_i=0|Y_i=1)P(Y_i)} {P(Z_i=0)}$

$=\frac{(1-p_i)\theta_i}{1-p_i\theta_i}$

Derivation of equation 4 :

If the fisherman is in cell $j$ then searching cell $i$ gives no
information about $j$ ,so we are guaranteed to not find the fisherman.

Therefore:

$P(Z_i=0|Y_j=1)=1$

$P(Y_j=1)=(\theta_j)^1(1-\theta_j)^0$

$=\theta_j$

As proved above :

$P(Z_i=0)=$ $1-p_i\theta_i$

Therefore:

$\theta_{j,new}=P(Y_j=1|Z_i=0) =\frac{P(Z_i=0|Y_j=1)P(Y_j)} {P(Z_i=0)}$

$\theta{j,new}=\frac{\theta_{j,old}}{1-p_i\theta_{i,old}}$

## b)

Equation 2 is the posterior probability the fisherman is in the cell
given that we fail to detect him.Although we don't detect the fisherman
he may still be in the cell.So we reduce the probability of occurring in
the cell rather than ruling it out.To show this decrease in probability
over time we update the new prior (probability of occurrence
$\theta_{i,new}$) using the old prior( probability of
occurence$\theta_{i,old}$).Therefore Equation 3 shows how the occurrence
probability is adjusted over time as we gain more evidence through the
bayesian search.

## c)

### Approach:

1.  **Initialize Prior and True Location using Jakes provided
    functions:**

    -   Generate the initial prior probability distribution using
        `generate_lost()`.
    -   Generate the true location of the fisherman using
        `generate_fisherman()`.
    -   Store the fisherman's coordinates as `rowf` and `colf`.

2.  **Variable initialization:**

    -   Create a posterior tracker vector of size 48 to record the
        posterior probability of the fisherman’s true location at each
        time step.
    -   Set posterior equal to the initial prior.
    -   Initialise a boolean called fishermanfound to false initially
        and use this to track whether the fisherman is found or not.

3.  **Create a for loop which loops through the amount of hours(48):**

    -   Merge the "prior" and detection probability to create a search
        grid
    -   Select the cell with the highest probability of successful
        detection
    -   If the chosen cell is the fisherman’s true location, simulate
        detection using the detection probability associated with the
        cell of interest(use rbinom).
    -   Otherwise, detection is automatically set to 0.
    -   Record the current posterior probability of the true location in
        `post_tracker[i]`.

4.  **Update Posterior if Fisherman Not Found:**

    If the fisherman is not detected use Bayes theorem to update the
    probability theorem:

    -   Update the probability in the searched cell using: $$
        \theta_{i,\text{new}} = \frac{(1 - p_i)\theta_{i,\text{old}}}{1 - p_i\theta_{i,\text{old}}}
        $$
    -   Update all other cells: $$
        \theta_{j,\text{new}} = \frac{\theta_{j,\text{old}}}{1 - p_i\theta_{i,\text{old}}} \quad \text{for } j \ne i
        $$

5.  **Successful Detection:**

    -   If detection is successful, break the loop and set the boolean
        fisherman found to true.

```{r template, include=FALSE}
#### Required Functions provided by Jake####
set.seed(123)

#### Data Generating Functions ####

generate_lost <- function(grid_size, nsims){
  
  # Function to generate the prior distribution for the 
  # location of the lost fisherman
  # Args: 
  #       grid_size: the dimensions of the square search grid
  #       nsims: number of samples to base the prior distribution on
  
  mu_vec  <- c(grid_size/2, grid_size/2)
  sig_mat <- matrix(c(2, 1, 5, 5), 2,2)
  
  dat <- mvrnorm(nsims, mu_vec, sig_mat)
  dat <- round(abs(dat))
  head(dat)
  summary(as.data.frame(dat))
  prior <- matrix(rep(0,grid_size^2), grid_size, grid_size)
  for (i in 1:NROW(dat)){
    
    if (dat[i,1] < grid_size & dat[i,2] < grid_size){
      prior[dat[i,1], dat[i,2]] <- prior[dat[i,1], dat[i,2]] + 1
    }
    
  }
  prior <- prior/sum(prior)
  return(prior)
}

generate_fisherman <- function(grid_size){
  
  # Function to generate the true location of the lost fisherman.
  # This should not effect the search decision in any way!! It is unkown
  # to the search crew.
  # Args: 
  #       grid_size: the dimensions of the square search grid

  mu_vec  <- c(grid_size/2, grid_size/2)
  sig_mat <- matrix(c(2, 1, 5, 5), 2,2)
  
  location  <- round(mvrnorm(1, mu_vec, sig_mat))
  true_grid <- matrix(rep(0, grid_size^2), grid_size, grid_size)
  true_grid[location[1], location[2]] <- 1
  
  return(true_grid)
}

#### Simulation ####

search_size <- 20
unifs <- runif(search_size^2, min = 0.6, max = 0.9)
##Step 2 - gain detection distribution
detect_pr <- matrix(unifs, ncol = search_size)

```

```{r prior,include=FALSE}
##step 1 - prior of experts
prior <- generate_lost(search_size,10000)
#get fishermans location 
fish_grid <- generate_fisherman(search_size)
cell <- which(fish_grid == 1,arr.ind=TRUE)
rowf <- cell[1]
colf <- cell[2]

max_hours <- 48
post_tracker <- numeric(max_hours)
fishermanfound <- FALSE
prior1 <- prior
###at the start your prior is your posterior
posterior <- prior

```

```{r posterior, include=FALSE}
for (i in 1:max_hours){
  ##step 3 - merge 2 distributions  
  ##posterior initially prior ,we use the previous posterior as the new prior
  grid <- posterior*detect_pr
  ##step 4 - search cell with highest probability
  #cell_check <- which(grid == 1,arr.ind=TRUE)
  ##finding the index with the highest probability
  cell_check <- arrayInd(which.max(grid), dim(grid))
  row <- cell_check[1]
  col <- cell_check[2]
 ##check if fisherman is in the cell
  if(fish_grid[row,col]==1){
    ##use detection pr check if we find fisherman given he is in the cell.
    ##1 sample,1 trial using highest probability of cells
    detected <- rbinom(1, 1, detect_pr[row, col])
  }
    else {
      detected <- 0
    }
  post_tracker[i] <- posterior[rowf,colf]
  
  #step 5 - if not detected update using Bayes
  if (detected == 1) {
    #cat("Fisherman here", i, "in cell (", row, ",", col, ")\n")
    fishermanfound <- TRUE
    break
  }
  pi <- detect_pr[row,col]
  posterior[row,col] <- ((1-pi)*posterior[row,col])/(1 - pi * posterior[row, col])
  posterior[-row, -col] <- posterior[-row, -col] / (1 - pi * posterior[row, col])
  ##normalising posterior values
  posterior <- posterior/sum(posterior)
}
```

```{r heatmap,echo=FALSE,message=FALSE, warning=FALSE, fig.fullwidth=TRUE}
##creating dummy variable which allows us to indicate the true location of the fisherman on our plots
fishermancell <- data.frame(x=colf,y=rowf)
fishermancell$label <- "True location"
df_prior <- expand.grid(x = 1:search_size, y = 1:search_size)
df_prior$value <- as.vector(prior1)
plot1 <- ggplot(df_prior, aes(x = x, y = y, fill = value)) +
  geom_tile() +
  scale_fill_gradient(low = "red", high = "blue") +
  coord_fixed() +
  geom_tile(data = fishermancell, aes(x = x, y = y,color=label),
            fill = NA, size = 0.5, show.legend = TRUE)+scale_color_manual(name = "", values = c("True location" = "yellow")) +
  labs(x='Column',y="Row",subtitle="First step", fill = "Probability") +
  theme_minimal()+theme(
    plot.margin = margin(t = 10, r = 10, b = 10, l = 10)
  )


df_posterior <- expand.grid(x = 1:search_size, y = 1:search_size)
df_posterior$value <- as.vector(posterior)
plot2 <- ggplot(df_posterior, aes(x = x, y = y, fill = value)) +
  geom_tile() +
  scale_fill_gradient(low = "red", high = "blue") +
  coord_fixed() +
  geom_tile(data = fishermancell, aes(x = x, y = y,color=label),
            fill = NA, size = 0.5, show.legend = TRUE)+scale_color_manual(name = "", values = c("True location" = "yellow")) +
  labs(x='Column',y="Row", subtitle="Final step",fill = "Probability") +
  theme_minimal()+theme(
    plot.margin = margin(t = 10, r = 10, b = 10, l = 10)
  )
##combining two plots into one for easier comparison

full_plot <- (plot1 + plot2) +
  plot_layout(ncol = 2,guides = "collect") +  
  plot_annotation(
    title = "Posterior probabilities of occurrence during Bayesian search",
    theme = theme(plot.title = element_text(
        hjust = 0.5,
        size = 12,
        face = "bold",
        margin = margin(b = 15)  ),
      legend.position = "right",legend.box.margin = margin(l = 20), 
      plot.margin = margin(t = 20, r = 10, b = 10, l = 10))
  )

full_plot

```

```{r search, echo=FALSE}
df_post_time <- data.frame(
  Time = 1:48,
  Posterior = post_tracker
)
ggplot(df_post_time, aes(x = Time, y = Posterior)) +
  geom_line(color = "blue", size = 1) +
  labs(
    title ="Posterior Probability at the true location",
    x = "Hours", 
    y = "Posterior Probability"
  ) +
  theme_minimal()

```

## d)

If $p_i$ is constant across cells, detection probability no longer
varies by location. In this case, the search strategy would focus on
cells with the highest prior probability, rather than the highest
probability of successful detection. If detection fails we still update
the posterior probability using Bayes Theorem and the occurrence
probabilities would also still need to be updated.Therefore the only
notable change would be which cell we search.

# Question 3

### A Twist on Linear Regressions

Consider the linear regression model where the data is generated as:

$$
Y_i =
\begin{cases}
x_i^\top \beta + e_i, & e_i \sim \mathcal{N}(0, \sigma_1^2), \quad i \in I_1 \\
x_i^\top \beta + e_i, & e_i \sim \mathcal{N}(0, \sigma_2^2), \quad i \in I_2
\end{cases}
$$

where: (I) is an index set. $(\sigma_1^2 < \sigma_2^2)$

This implies that observations of the second index set have a higher variance than those of the first, potentially representing outliers in the dataset.

As usual, let $\tau_i = \frac{1}{\sigma_i^2}$. If we assume that (I_1) is known, then without loss of generality (w.l.o.g), we can let:

$$
I_1 = \{1, \ldots, n_1\}, \quad I_2 = \{n_1 + 1, \ldots, n\}
$$

The likelihood for the data is as follows:

$$
L(\mathbf{Y} \mid \boldsymbol{\beta}, \tau_1, \tau_2) =
\prod_{i \in I_1} \left( \frac{\tau_1^{1/2}}{\sqrt{2\pi}} \exp\left( -\frac{\tau_1}{2} (Y_i - \mathbf{x}_i^\top \boldsymbol{\beta})^2 \right) \right)
\prod_{i \in I_2} \left( \frac{\tau_2^{1/2}}{\sqrt{2\pi}} \exp\left( -\frac{\tau_2}{2} (Y_i - \mathbf{x}_i^\top \boldsymbol{\beta})^2 \right) \right)
$$

This simplifies to:

$$
L(\mathbf{Y} \mid \boldsymbol{\beta}, \tau_1, \tau_2) \propto \tau_1^{n_1/2} \cdot \tau_2^{n_2/2} \cdot
\exp\left(
-\frac{\tau_1}{2} (\mathbf{Y}_1 - \mathbf{X}_1 \boldsymbol{\beta})^\top (\mathbf{Y}_1 - \mathbf{X}_1 \boldsymbol{\beta})
- \frac{\tau_2}{2} (\mathbf{Y}_2 - \mathbf{X}_2 \boldsymbol{\beta})^\top (\mathbf{Y}_2 - \mathbf{X}_2 \boldsymbol{\beta})
\right)
$$

where:

$$
\Sigma^{-1} = \frac{1}{\det(\text{Data Matrix})} \cdot \operatorname{adj}(\text{Data Matrix})
$$

$$
\Sigma^{-1} =
\begin{bmatrix}
\tau_1 & 0 & 0 & 0 \\
0 & \cdots & 0 & 0 \\
0 & \tau_1 & 0 & 0 \\
0 & 0 & \tau_2 & 0 \\
0 & 0 & \cdots & 0 \\
0 & 0 & 0 & \tau_2
\end{bmatrix}
$$

$$
\Sigma^{-1} =
\tau_1
\underbrace{
\begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & \cdots & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & \cdots & 0 \\
0 & 0 & 0 & 0
\end{bmatrix}
}_{\text{Matrix A}}
+
\tau_2
\underbrace{
\begin{bmatrix}
0 & 0 & 0 & 0 \\
0 & \cdots & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & \cdots & 0 \\
0 & 0 & 0 & 1
\end{bmatrix}
}_{\text{Matrix B}}
$$

### Posterior Derivations

Case 1: Posterior of $\boldsymbol{\beta} \mid \tau_1, \tau_2, \boldsymbol{X}, \boldsymbol{Y}$

Given the following prior distribution:
$$\boldsymbol{\beta} \sim \mathcal{N}(\mathbf{0}, \mathbf{T}_0)$$
which has the unnormalized form:
$$\pi(\boldsymbol{\beta}) \propto \exp\left( -\frac{1}{2} \boldsymbol{\beta}^\top \mathbf{T}_0^{-1} \boldsymbol{\beta} \right)$$
The proportional likelihood is given by:
$$L(\boldsymbol{\beta}, \mathbf{Y}, \mathbf{X}, \Sigma) \propto \exp\left( -\frac{1}{2} \boldsymbol{\beta}^\top \mathbf{X}^\top \Sigma^{-1} \mathbf{X} \boldsymbol{\beta} \right)$$
Therefore, the posterior is:
$$\pi(\boldsymbol{\beta}, \tau_1, \tau_2, \boldsymbol{X}, \boldsymbol{Y}) 
\propto \pi(\boldsymbol{\beta}) \cdot L(\boldsymbol{\beta}, \boldsymbol{Y}, \boldsymbol{X}, \Sigma) 
\propto \exp\left( -\frac{1}{2} \boldsymbol{\beta}^\top T_0^{-1} \boldsymbol{\beta} \right) 
\cdot \exp\left( -\frac{1}{2} \boldsymbol{\beta}^\top \boldsymbol{X}^\top \Sigma^{-1} \boldsymbol{X} \boldsymbol{\beta} \right)$$
$$\mathbf{C} = \mathbf{X}^\top \Sigma^{-1} \mathbf{X} + \mathbf{T}_0^{-1}, \quad
\mathbf{D} = \mathbf{X}^\top \Sigma^{-1} \mathbf{Y}$$
$$\boldsymbol{\beta}^\top \mathbf{C} \boldsymbol{\beta} - 2 \boldsymbol{\beta}^\top \mathbf{D} = (\boldsymbol{\beta} - \boldsymbol{\mu}_\beta)^\top \mathbf{C} (\boldsymbol{\beta} - \boldsymbol{\mu}_\beta) - \boldsymbol{\mu}_\beta^\top \mathbf{C} \boldsymbol{\mu}_\beta$$
where:
$$\boldsymbol{\mu}_\beta = \mathbf{C}^{-1} \mathbf{D}$$
The posterior becomes:
$$\pi(\boldsymbol{\beta} \mid \tau_1, \tau_2, \mathbf{X}, \mathbf{Y}) \propto
\exp\left( -\frac{1}{2} (\boldsymbol{\beta} - \boldsymbol{\mu}_\beta)^\top \mathbf{C} (\boldsymbol{\beta} - \boldsymbol{\mu}_\beta) \right)$$
But:
$\exp\left( -\frac{1}{2} \mu_B^\top C \mu_B \right) \quad \text{is independent of } \boldsymbol{\beta}$

Therefore, the posterior simplifies to: $\pi(\boldsymbol{\beta} \mid \tau_1, \tau_2, \boldsymbol{X}, \boldsymbol{Y}) \propto \exp\left( -\frac{1}{2} (\boldsymbol{\beta} - \mu_B)^\top \left( \boldsymbol{X}^\top \Sigma^{-1} \boldsymbol{X} + T_0^{-1} \right) (\boldsymbol{\beta} - \mu_B) \right)$

This is a Gaussian: $\pi(\boldsymbol{\beta} \mid \tau_1, \tau_2, \boldsymbol{X}, \boldsymbol{Y}) \sim \mathcal{N}(\boldsymbol{\mu_\beta}, \boldsymbol{C}^{-1})$

where:
$$\boldsymbol{C}^{-1} = \left( \boldsymbol{X}^\top \Sigma^{-1} \boldsymbol{X} + T_0^{-1} \right)^{-1} \quad
\boldsymbol{\mu_\beta} = \left( \boldsymbol{X}^\top \Sigma^{-1} \boldsymbol{Y} \right) \times \left( \boldsymbol{X}^\top \Sigma^{-1} \boldsymbol{X} + T_0^{-1} \right)$$

For Case 2:) Posterior of $\tau_1 | \boldsymbol{\beta}, \tau_2, \boldsymbol{X}, \boldsymbol{Y}$

Given the following prior distribution:$\tau_1 \sim \text{Gamma}(a, b)$

which has the unnormalized form:
$$\pi(\tau_1) \propto \tau_1^{a - 1} \exp(-b \tau_1)$$
The likelihood contribution for group 1 ( n1) is:
$$L(Y_1, X_1, \boldsymbol{\beta}, \tau_1) \propto \tau_1^{n_1 / 2} \exp\left[ -\frac{\tau_1}{2} (Y_1 - X_1 \beta)^\top (Y_1- X_1 \beta) \right].$$
Therefore, the posterior is:
$\pi(\tau_1 \mid \boldsymbol{\beta}, \tau_2, \boldsymbol{X}, \boldsymbol{Y}) \propto \pi(\tau_1 ) \cdot L(Y_1, X_1, \boldsymbol{\beta}, \tau_1),$

$\pi(\tau_1 \mid \boldsymbol{\beta}, \tau_2, \boldsymbol{X}, \boldsymbol{Y}) \propto \tau_1^{a - 1} \exp(-b \tau_1) \cdot \tau_1^{n_1 / 2} \exp\left( -\frac{\tau_1}{2} (Y_1 - X_1 \beta)^\top (Y_1 - X_1 \beta) \right),$

$\pi(\tau_1 \mid \boldsymbol{\beta}, \tau_2, \boldsymbol{X}, \boldsymbol{Y}) \propto \tau_1^{a + n_1 / 2 - 1} \exp\left( -\tau_1 \left( b + \frac{1}{2} (Y_1 - X_1 \beta)^\top (Y_1 - X_1 \beta) \right) \right) .$

The normalized form is: $\pi(\tau_1 \mid \beta, \tau_2, X, Y) \sim \text{Gamma}\left(a + \frac{n_1}{2}, b + \frac{1}{2}(Y_1 - X_1 \beta)^\top (Y_1 - X_1 \beta) \right )$

Case 3: Posterior of $\tau_2 \mid \tau_1, \boldsymbol{\beta}, \boldsymbol{X}, \boldsymbol{Y}$

Given the following prior distribution:

$\tau_2 \mid \tau_1 \sim \text{Gamma}(a, b) \cdot \mathbb{I}(\tau_2 < \tau_1),$

which has the unnormalized form:

$\pi(\tau_2 \mid \tau_1) \propto \tau_2^{a - 1} \exp(-b \tau_2) \cdot \mathbb{I}(\tau_2 < \tau_1)$

The likelihood contribution for group 2 (n2 = n - n1) is:

$L(Y_2, X_2, \boldsymbol{\beta}, \tau_2) \propto \tau_2^{n_2 / 2} \exp\left[ -\frac{\tau_2}{2} (Y_2 - X_2 \beta)^\top (Y_2 - X_2 \beta) \right]$

Therefore, the posterior is:

$\pi(\tau_2 \mid \boldsymbol{\beta}, \tau_1, \boldsymbol{X}, \boldsymbol{Y}) \propto \pi(\tau_2 \mid \tau_1) \cdot L(Y_2, X_2, \boldsymbol{\beta}, \tau_2),$

$\pi(\tau_2 \mid \boldsymbol{\beta}, \tau_1, \boldsymbol{X}, \boldsymbol{Y}) \propto \tau_2^{a - 1} \exp(-b \tau_2) \cdot \tau_2^{n_2 / 2} \exp\left( -\frac{\tau_2}{2} (Y_2 - X_2 \beta)^\top (Y_2 - X_2 \beta) \right) \cdot \mathbb{I}(\tau_2 < \tau_1),$

$\pi(\tau_2 \mid \boldsymbol{\beta}, \tau_1, \boldsymbol{X}, \boldsymbol{Y}) \propto \tau_2^{a + n_2 / 2 - 1} \exp\left( -\tau_2 \left( b + \frac{1}{2} (Y_2 - X_2 \beta)^\top (Y_2 - X_2 \beta) \right) \right) \cdot \mathbb{I}(\tau_2 < \tau_1)$

The normalized form is: $\pi(\tau_2 \mid \beta, \tau_1, X, Y) \sim \text{Truncated-Gamma}\left(a + \frac{n_2}{2}, b + \frac{1}{2}(Y_2 - X_2 \beta)^\top (Y_2 - X_2 \beta) \right )\cdot \mathbb{I}(\tau_2 < \tau_1)$

```{r q3, echo = FALSE}
set.seed(2021)

# Simulate data (from your code)
n = 150
X.c = data.frame(matrix(rnorm(5 * n), ncol = 5))
colnames(X.c) = c("X1", "X2", "X3", "X4", "X5")
X = as.matrix(cbind(1, X.c))  # Add intercept
e = matrix(rnorm(n), ncol = 1)
beta.true = matrix(c(1, 0, 10, 0, 2, -3), ncol = 1)
Y = X %*% beta.true + e


# Spliting the data into n1 and n-n1 is = n2 for the distributions
n1 <- 100  
n2 <- n - n1

#Spliting the data into X1 and X2 for the distributions
X1 <- X[1:n1, ] 
X2 <- X[(n1+1):n, ]

#Spliting the data into Y1 and Y1 for the distributions
Y1 <- Y[1:n1, ]
Y2 <- Y[(n1+1):n, ]

#Initial values for a and b 
a <- 2
b <- 1


# Priors distributions
number_col <- ncol(X)
T0 <- diag(1000, number_col )  # Prior covariance for beta

#Using 3000 iterations for sample generation
iterations <- 3000
beta_samples <- matrix(0, nrow = iterations, ncol = number_col )
tau1_samples <- numeric(iterations)
tau2_samples <- numeric(iterations)

beta_parameter <- rep(0, number_col )
tau1 <- 1
tau2 <- 0.5

#  sampling  from the posterior using Gibbs sampling
for (iter in 1:iterations) {
  # Sample tau1 | beta,X,Y,tau2
  sigma_1 <- Y1 - X1 %*% beta_parameter
  shape_parameter1 <- a + n1 / 2
  rate_parameter_1 <- b + 0.5 * sum(sigma_1^2)
  tau1 <- rgamma(1, shape =  shape_parameter1, rate = rate_parameter_1)
  
  #Sample beta | tau1, tau2, X, Y
  C <- tau1 * t(X1) %*% X1 + tau2 * t(X2) %*% X2 + solve(T0)
  C_inverse <- solve(C)
  mu_parameter <- C_inverse %*% (tau1 * t(X1) %*% Y1 + tau2 * t(X2) %*% Y2)
  beta_parameter <- as.vector(mvrnorm(1, mu = mu_parameter, Sigma = C_inverse))
  
  
  # Sample tau2 | beta, tau1( where n2 = n-n1)
  sigma_2 <- Y2 - X2 %*% beta_parameter
  shape_parameter2<- a + n2 / 2
  rate_parameter_2 <- b + 0.5 * sum(sigma_2^2)
  tau2 <- rgamma(1, shape = shape_parameter2, rate = rate_parameter_2 )

  # To get posterior samples
  beta_samples[iter, ] <- beta_parameter
  tau1_samples[iter] <- tau1
  tau2_samples[iter] <- tau2
}

col_labels <- c("$\\beta_0$", "$\\beta_1$", "$\\beta_2$", 
					 "$\\beta_3$", "$\\beta_4$", "$\\beta_5$")

kable(head(beta_samples), 
		digits = 3, 
		caption = 'First six rows of the $\\beta$ sample values', 
		col.names = col_labels)

kable(head(tau1_samples), 
		digits = 3,
		caption = 'First six rows of $\\tau_1$ sample values', 
		col.names = c('$\\tau_1$'))

kable(head(tau2_samples), 
		digits = 3,
		caption = 'First six rows of $\\tau_2$ sample values', 
		col.names = c('$\\tau_2$'))
```

# Plagiarism Declaration
## Jorge Abrahams Laylaa (JRGLAY001) declaration

\begin{enumerate}

\item [1.] I know that plagiarism is a serious form of academic misconduct.
\item [2.] I have read the document about avoiding plagiarism, am familiar with its contents and have avoided all forms of plagiarism mentioned there.
\item [3.] Where I have used the words of others, I have indicated this by the use of quotation marks.
\item [4.] I have referenced all quotations and properly acknowledged ideasborrowed from others.
\item [5.] I have not and shall not allow others to plagiarise my work.
\item [6.] I declare that this is my own work.
\item [7.] I am attaching the summary of the Turnitin match overview (when required to do so).

\end{enumerate}

```{=latex}
\begin{flushleft}
\includegraphics[width=0.15\textwidth]{Signature_Laylaa.jpg}\\
Signature: Jorge Abrahams Laylaa (JRGLAY001)
\end{flushleft}
```

## Gumede Sbonelo (GMDSBO006) declaration

\begin{enumerate}

\item [1.] I know that plagiarism is a serious form of academic misconduct.
\item [2.] I have read the document about avoiding plagiarism, am familiar with its contents and have avoided all forms of plagiarism mentioned there.
\item [3.] Where I have used the words of others, I have indicated this by the use of quotation marks.
\item [4.] I have referenced all quotations and properly acknowledged ideasborrowed from others.
\item [5.] I have not and shall not allow others to plagiarise my work.
\item [6.] I declare that this is my own work.
\item [7.] I am attaching the summary of the Turnitin match overview (when required to do so).

\end{enumerate}

```{=latex}
\begin{flushleft}
\includegraphics[width=0.15\textwidth]{Signature_Sbonelo.jpg}\\
Signature: Gumede Sbonelo (GMDSBO006)
\end{flushleft}
```

## Mugeri Khethani (MGRKHE001) declaration

\begin{enumerate}

\item [1.] I know that plagiarism is a serious form of academic misconduct.
\item [2.] I have read the document about avoiding plagiarism, am familiar with its contents and have avoided all forms of plagiarism mentioned there.
\item [3.] Where I have used the words of others, I have indicated this by the use of quotation marks.
\item [4.] I have referenced all quotations and properly acknowledged ideasborrowed from others.
\item [5.] I have not and shall not allow others to plagiarise my work.
\item [6.] I declare that this is my own work.
\item [7.] I am attaching the summary of the Turnitin match overview (when required to do so).

\end{enumerate}

```{=latex}
\begin{flushleft}
\includegraphics[width=0.12\textwidth]{Signature_Khethani.jpg}\\
Signature: Mugeri Khethani (MGRKHE001)
\end{flushleft}
```